{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HWhT0xjvitJp"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dbamman/nlp22/blob/main/HW4/HW_4.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "inWOGaHZMsUH"
      },
      "source": [
        "# Homework 4: Language Modeling\n",
        "\n",
        "In this homework, we will explore implementations of various language models we saw in lecture. We will use a dataset of movie reviews to learn statitics about words in our language to build our models. We will build classical N-Gram Models, measure perplexity, and generate text using the models."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iNBJieAJcdlY"
      },
      "source": [
        "## Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "xJy9Ar2YFz6h",
        "outputId": "1ad05a57-0eb2-4772-87b2-969e5579d223",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        }
      ],
      "source": [
        "from collections import Counter\n",
        "import numpy as np\n",
        "import math\n",
        "import tqdm\n",
        "import random\n",
        "\n",
        "import pandas as pd\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "nltk.download('punkt')\n",
        "\n",
        "import torch\n",
        "from torch import nn\n",
        "import torch.nn.functional as F\n",
        "import torchtext.legacy as torchtext"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "EDW95kiEPWim",
        "outputId": "f509d3a2-76a1-4bbc-8603-ae85cdf4a185",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295,
          "referenced_widgets": [
            "2bf4343c137c403190dafe0cba69a399",
            "75ff119b5d834ac19da87c7e5f3501c5",
            "96c5c17490024cc6a6ce012c306719fb",
            "5263766dc3514d359c1181d5c2d48502",
            "568f8576aa7d4ca5b50d79d8115504eb",
            "f4c51b7caf4844e19aa273e7446a1469",
            "923aa292540c4e68b84985395bacc8f8",
            "f6433befc90f46d884d38acc0740b666",
            "e87f38713c044707bb71fa7b0851236b",
            "ba55617e8b0e4c728c6cff99de8f3dd6",
            "a5c41eef61f74694932d3ba90d80b608"
          ]
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2022-02-14 00:04:28--  https://raw.githubusercontent.com/dbamman/nlp22/main/HW4/plot_summaries.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 75934033 (72M) [text/plain]\n",
            "Saving to: ‘plot_summaries.txt’\n",
            "\n",
            "plot_summaries.txt  100%[===================>]  72.42M   135MB/s    in 0.5s    \n",
            "\n",
            "2022-02-14 00:04:28 (135 MB/s) - ‘plot_summaries.txt’ saved [75934033/75934033]\n",
            "\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "2bf4343c137c403190dafe0cba69a399",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "  0%|          | 0/42303 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of words in the vocabulary: 57663\n",
            "Example text: ['he', 'stumbles', 'upon', 'a', 'jailbreak', 'and', 'knocks', 'out', 'the', 'convicts', '.', 'he', 'is', 'hailed', 'a', 'hero', 'and', 'is', 'released', '.', 'outside', 'the', 'jail', ',', 'he', 'discovers', 'life', 'is', 'harsh', ',']\n"
          ]
        }
      ],
      "source": [
        "# download and load the data\n",
        "!wget https://raw.githubusercontent.com/dbamman/nlp22/main/HW4/plot_summaries.txt\n",
        "data = pd.read_csv('plot_summaries.txt', sep='\\t', header=None)[1].tolist()\n",
        "\n",
        "def make_text(data):\n",
        "    all_text = []\n",
        "    for d in tqdm.notebook.tqdm(data):\n",
        "        all_text.append(\"<eos>\")\n",
        "        clean_text = \" \".join(d.lower().split())\n",
        "        all_text.extend(word_tokenize(clean_text))\n",
        "        all_text.append(\"<eos>\")\n",
        "    return all_text\n",
        "\n",
        "text = make_text(data)\n",
        "train_size, validation_size = 2000000, 200000\n",
        "train_text, validation_text = text[:train_size], text[train_size:train_size+validation_size]\n",
        "\n",
        "text_field = torchtext.data.Field()\n",
        "counter = Counter(train_text)\n",
        "text_field.vocab = text_field.vocab_cls(counter)\n",
        "vocab = text_field.vocab\n",
        "vocab_size = len(vocab)\n",
        "\n",
        "print(\"Number of words in the vocabulary: {}\".format(vocab_size))\n",
        "print(\"Example text: {}\".format(validation_text[:30]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "bYh1e5jYy5YL"
      },
      "outputs": [],
      "source": [
        "def make_dataset(text):\n",
        "    a = torchtext.data.Example.fromlist([\" \".join(text)], [('text', text_field)])\n",
        "    return torchtext.data.Dataset([a], [('text', text_field)])\n",
        "\n",
        "train_dataset, validation_dataset = make_dataset(train_text), make_dataset(validation_text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "SCcz1iY5Pd4v"
      },
      "outputs": [],
      "source": [
        "def ids(vocab, tokens):\n",
        "    \"\"\"Helper function to convert a list of words into indices in our vocab\"\"\"\n",
        "    return [vocab.stoi[t] for t in tokens]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "luLaQKJxitKk",
        "outputId": "b289ff2f-78ca-4688-c109-538f20bfbdc9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Running on cuda\n"
          ]
        }
      ],
      "source": [
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(\"Running on {}\".format(device))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xuyB3RfjPoGu"
      },
      "source": [
        "## Classical N-Gram Model\n",
        "\n",
        "For this part, we will build a classical N-Gram model to learn the statistics of our training data. To train this model, we simply count the number of times each $n$-gram occurs in our training text and divide it by the number of times the first $n-1$ words occur. Additionally, we will also add alpha-smoothing to make sure no word has $0$ probability. This is summed up in this equation:\n",
        "\n",
        "$$P(w_n|w_1 \\dots w_{n-1})=\\frac{C(w_1 \\dots w_n)+\\alpha}{C(w_1 \\dots w_{n-1})+\\alpha\\cdot|V|}$$\n",
        "\n",
        "where $|V|$ is the vocab size and $C$ is the count for the given n-gram.\n",
        "\n",
        "We will handle computing the counts for you and your job will be to simply fill in the functions to compute the above equation and the perplexity for the model."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dd = [\"sfsf\",\"dfdfdf\",\"dfd\"]\n",
        "dd[0:2]"
      ],
      "metadata": {
        "id": "HKCQqebj0RgM",
        "outputId": "b5184f8f-d00e-4ab3-a83f-16d7534494e5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['sfsf', 'dfdfdf']"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dd = [\"sfsf\",\"dfdfdf\",\"dfd\"]\n",
        "new = []\n",
        "n=3\n",
        "for i in range(len(dd)-1):\n",
        "  if n==len(dd):\n",
        "    new.append(dd[i:i+n])\n",
        "    break\n",
        "  else:\n",
        "    new.append(dd[i:i+n])\n",
        "print(new)"
      ],
      "metadata": {
        "id": "OhSsraiovY-C",
        "outputId": "c400b002-61ea-4dba-86bd-f87fd3a4d327",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[['sfsf', 'dfdfdf', 'dfd']]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "curr = [\"<eos>\"] * (3 - 1)\n",
        "curr"
      ],
      "metadata": {
        "id": "nSdRkati03wb",
        "outputId": "784020aa-2c68-4fd6-94be-3b2f43d79450",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['<eos>', '<eos>']"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\" \".join(new[0][:(len(new[0])-1)])"
      ],
      "metadata": {
        "id": "4B0MbhfU5ZIM",
        "outputId": "f51e0c41-1b99-446a-ccdf-5e8a5b2504ce",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'sfsf dfdfdf'"
            ]
          },
          "metadata": {},
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(5,0,-1):\n",
        "  print(i)"
      ],
      "metadata": {
        "id": "YCMw60Cm3qXp",
        "outputId": "83bfbf9f-0b2b-4ec7-9330-d557c9dee9e3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "5\n",
            "4\n",
            "3\n",
            "2\n",
            "1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "CGL9NjNK478q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "0xJjoXN_F-s0",
        "outputId": "2855312f-b634-4cb2-c940-e8bacdcbf0ed",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 131
        }
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "IndentationError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-14-db325d458cd2>\"\u001b[0;36m, line \u001b[0;32m48\u001b[0m\n\u001b[0;31m    (self.counts_n)/(self.counts_n_1)\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mIndentationError\u001b[0m\u001b[0;31m:\u001b[0m expected an indented block\n"
          ]
        }
      ],
      "source": [
        "class NGramModel:\n",
        "    def __init__(self, train_text, vocab, n=2, alpha=3e-3):\n",
        "        # get counts and perform any other setup\n",
        "        self.n = n\n",
        "        self.smoothing = alpha\n",
        "        self.vocab = vocab\n",
        "\n",
        "        # count n-grams\n",
        "        self.counts_n = Counter()\n",
        "        curr = [\"<eos>\"] * (self.n - 1) # padding for initial words\n",
        "        for i in range(len(train_text)):\n",
        "            curr.append(train_text[i])\n",
        "            gram = \" \".join(curr)\n",
        "            self.counts_n[gram] += 1\n",
        "            curr.pop(0)\n",
        "\n",
        "        # count n-1-gram\n",
        "        self.counts_n_1 = Counter()\n",
        "        for c in self.counts_n:\n",
        "          temp = \" \".join(c.split(\" \")[:-1])\n",
        "          self.counts_n_1[temp] += self.counts_n[c]  \n",
        "\n",
        "    def get_probability(self, text):\n",
        "        \"\"\"Return the probability of the last word in an n-gram. This is the\n",
        "        equation given in the text above.\n",
        "        \n",
        "        Args:\n",
        "            text: a list of string tokens\n",
        "\n",
        "        Hints:\n",
        "            - self.counts_n contains the number of occurances for any string of n words\n",
        "            - self.count_n_1 contains the number of occurances for any string of n-1 words\n",
        "            - you can use the join method to create a string from a list of words\n",
        "            - Remember, the given string can have more than n words\n",
        "            - self.vocab contains a dictionary of the vocabulary\n",
        "        \"\"\"\n",
        "        assert len(text) >= self.n, f\"Expected at least {self.n} words; got {len(text)} words. \\nGiven text: {text}\"\n",
        "\n",
        "        # BEGIN SOLUTION\n",
        "\n",
        "        # count instances of n grams/ count instances of n-1 grams\n",
        "\n",
        "        n_gram_n = \" \".join(text)\n",
        "\n",
        "        # get combinations of n grams\n",
        "        gram_comb = []\n",
        "\n",
        "        for i in range(len(text)-1):\n",
        "          if n==len(text):\n",
        "            gram_comb.append(text[i:i+n])\n",
        "            break\n",
        "          else:\n",
        "            gram_comb.append(text[i:i+n])\n",
        "\n",
        "        for comb in gram_comb:\n",
        "          join_n_1= \" \".join(comb[:(len(comb)-1)])\n",
        "          (self.counts_n[comb[-1]])/()\n",
        "\n",
        "\n",
        "        prob = (self.counts_n[text[-1]])/self.\n",
        "\n",
        "          text[i]\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "        \n",
        "        \n",
        "        (self.counts_n)/(self.counts_n_1) \n",
        "\n",
        "\n",
        "\n",
        "\n",
        "        \n",
        "        # END SOLUTION\n",
        "\n",
        "    def get_next_word_probabilities(self, text_prefix):\n",
        "        \"\"\"Return a list of probabilities for each word in the vocabulary.\n",
        "\n",
        "        Args:\n",
        "            text_prefix: a list of string tokens\n",
        "\n",
        "        Hints:\n",
        "            - you need to use your get_probability function\n",
        "            - self.vocab.itos contains a list of words to return probabilities for\n",
        "            - you will need to handle the cases in which the text prefix is both\n",
        "                shorter and longer than n-1 words. For the shorter case, you need to\n",
        "                pad with \"<eos>\" tokens to the beginning of the text prefix. For the\n",
        "                longer case, you need to truncate the text prefix to the last n-1 words\n",
        "            - As a sanity check, you should make sure the probabilities you return add up to 1\n",
        "        \"\"\"\n",
        "        # BEGIN SOLUTION\n",
        "        \n",
        "        # END SOLUTION"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pR5BV_IbFNbF"
      },
      "source": [
        "### Perplexity\n",
        "\n",
        "To evaluate how good our language model, we use a metric called perplexity. The perplexity of a language model (PP) on a test set is the inverse probability of the test set, normalized by the number of words. Let $W = w_{1}w_{2}\\dots w_{N}$. Then,\n",
        "\n",
        "$$PP(W) = \\sqrt[N]{\\prod_{i = 1}^{N}\\frac{1}{P(w_{i}|w_{1}\\dots w_{i - 1})}}$$\n",
        "\n",
        "However, since these probabilities are often small, taking the inverse and multiplying can be numerically unstable, so we often first compute these values in the log domain and then convert back. So this equation looks like:\n",
        "\n",
        "$$\\ln PP(W) = \\frac{1}{N} \\sum_{i = 1}^{N} -\\ln P(w_{i}|w_{1}\\dots w_{i - 1})$$\n",
        "\n",
        "$$\\implies PP(W) = e^{\\frac{1}{N} \\sum_{i = 1}^{N} -\\ln P(w_{i}|w_{1}\\dots w_{i - 1})}$$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Qz99lOhKEPgT"
      },
      "outputs": [],
      "source": [
        "def get_perplexity(model, text):\n",
        "    \"\"\"Returns the perplexity of the model.\n",
        "    \n",
        "    Args:\n",
        "        text: a list of string tokens\n",
        "\n",
        "    Hints:\n",
        "        - you need to use your model.get_probability function\n",
        "        - you need to handle the edge case for the first n-1 words of text. You\n",
        "            can similarly pad with \"<eos>\" tokens\n",
        "        - you want to get the probability of each increasing sequence of words\n",
        "    \"\"\"\n",
        "    # BEGIN SOLUTION\n",
        "    \n",
        "    # END SOLUTION\n",
        "\n",
        "unigram_model = NGramModel(train_text, vocab, n=1)\n",
        "print('unigram validation perplexity:', get_perplexity(unigram_model, validation_text)) # should be around 1300-1400\n",
        "\n",
        "bigram_model = NGramModel(train_text, vocab, n=2)\n",
        "print('bigram validation perplexity:', get_perplexity(bigram_model, validation_text)) # should be around 800\n",
        "\n",
        "trigram_model = NGramModel(train_text, vocab, n=3)\n",
        "print('trigram validation perplexity:', get_perplexity(trigram_model, validation_text)) # this won't do very well (around 5000)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "79oQTkEMg-yG"
      },
      "source": [
        "### Deliverable 1\n",
        "\n",
        "Fill in the calculated perplexities given from the cell above and answer the question."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2i7Cgt1SgVbN"
      },
      "source": [
        "<!-- Do not remove this comment, it is used by the autograder: RqYJKsoTS6 -->\n",
        "\n",
        "Unigram validation perplexity: ***Your Answer Here***\n",
        "\n",
        "Bigram validation perplexity: ***Your Answer Here***\n",
        "\n",
        "Trigram validation perplexity: ***Your Answer Here***"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "23duN0Mkg7Zy"
      },
      "source": [
        "Question: Why does the trigram model have such a high perplexity?\n",
        "\n",
        "Answer: ***Your Answer Here***"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XBELAx_xcx4m"
      },
      "source": [
        "## Text Generation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eGQqecOwcNfW"
      },
      "source": [
        "### Deliverable 2\n",
        "\n",
        "In this section, we will explore generating sentences using our models. Your job will be to simply fill-in the following function. You should try out the various models you have built and compare the types of sentences they generate. These models are of course very limited compared to current state of the art models that are able to model much longer sequences of text."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tetguEyrHppc"
      },
      "outputs": [],
      "source": [
        "def generate_text(model, n=20, seed=0, prefix=['<eos>', '<eos>']):\n",
        "    \"\"\"Returns a randomly generated sentence sampled from the probability\n",
        "    distribution given by a language model.\n",
        "\n",
        "    Args:\n",
        "        model: language model\n",
        "        n: number of words to generate\n",
        "        prefix: list of tokens to prompt your model\n",
        "    \n",
        "    Hints:\n",
        "        - you need to use your model.get_next_word_probabilities function\n",
        "        - you can use the random.choices function to sample from a list according to probabilities\n",
        "        - model.vocab.itos contains a list of words in the vocabulary\n",
        "    \"\"\"\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    # BEGIN SOLUTION\n",
        "    \n",
        "    # END SOLUTION"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UK0ZF-gfcUOS"
      },
      "outputs": [],
      "source": [
        "generate_text(unigram_model)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dk5HfTLlCVBI"
      },
      "source": [
        "## Optional: Neural Models\n",
        "\n",
        "As you can see from the text you generated in the previous section, the capabilities of classical language models is quite limited. They simply learn the meanings of words in terms of counts and are limited to a fixed window of words. In this section, we will explore neural methods for language modeling, which are a bit closer to what modern language models look like.\n",
        "\n",
        "You don't have to do anything in section and there is no deliverable. It is merely here for you to explore how neural language models work. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TFw-oZhXcUId"
      },
      "source": [
        "### Neural N-Gram Model\n",
        "\n",
        "Now, we will train a neural network to model our language. We will use a feedforward network that takes in the previous $n-1$ words and outputs a distribution over the vocabulary which can be used to form a probability of the next word.\n",
        "\n",
        "We will implement this model using PyTorch and its various utilities for dataloading."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WuBVYDScI3w1"
      },
      "outputs": [],
      "source": [
        "class NgramDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, text_token_ids, n):\n",
        "        self.text_token_ids = text_token_ids\n",
        "        self.n = n\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.text_token_ids)\n",
        "\n",
        "    def __getitem__(self, i):\n",
        "        if i < self.n-1:\n",
        "            prev_token_ids = [vocab.stoi['<eos>']] * (self.n-i-1) + self.text_token_ids[:i]\n",
        "        else:\n",
        "            prev_token_ids = self.text_token_ids[i-self.n+1:i]\n",
        "\n",
        "        x = torch.tensor(prev_token_ids)\n",
        "        y = torch.tensor(self.text_token_ids[i])\n",
        "        return x, y\n",
        "\n",
        "class NeuralNGram(nn.Module):\n",
        "    def __init__(self, n):\n",
        "        super().__init__()\n",
        "        self.n = n\n",
        "\n",
        "        self.linear_1 = nn.Linear((self.n - 1) * 128, 256)\n",
        "        self.linear_2 = nn.Linear(256, 128)\n",
        "        self.dropout = nn.Dropout(p=0.1)\n",
        "        self.linear_3 = nn.Linear(128, vocab_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"Returns a tensor of log probabilities with shape (batch, vocab_size).\n",
        "\n",
        "        Args:\n",
        "            x: tensor of input with shape (batch, n-1)\n",
        "        \"\"\"\n",
        "        x = F.embedding(x, weight=self.linear_3.weight) # weight tying\n",
        "        x = x.reshape(x.size(0), -1)\n",
        "        x = self.linear_1(x)\n",
        "        x = F.relu(x)\n",
        "        x = self.dropout(x)\n",
        "        x = self.linear_2(x)\n",
        "        x = self.linear_3(x)\n",
        "        return x\n",
        "\n",
        "class NeuralNGramModel:\n",
        "    def __init__(self, n, vocab):\n",
        "        self.n = n\n",
        "        self.vocab = vocab\n",
        "        self.network = NeuralNGram(n).to(device)\n",
        "\n",
        "    def train(self):\n",
        "        dataset = NgramDataset(ids(self.vocab, train_text), self.n)\n",
        "        train_loader = torch.utils.data.DataLoader(dataset, batch_size=128, shuffle=True)\n",
        "        optim = torch.optim.Adam(self.network.parameters())\n",
        "        prev_validation = float(\"inf\")\n",
        "\n",
        "        for epoch in range(3):\n",
        "            print(\"Epoch\", epoch)\n",
        "            self.network.train()\n",
        "\n",
        "            for prev, curr in tqdm.notebook.tqdm(train_loader, leave=False):\n",
        "                prev, curr = prev.to(device), curr.to(device)\n",
        "                optim.zero_grad()\n",
        "                output = self.network(prev)\n",
        "                loss = F.cross_entropy(output, curr)\n",
        "                loss.backward()\n",
        "                optim.step()\n",
        "\n",
        "            # save the model with the best validation perplexity\n",
        "            validation_pp = get_perplexity(self, validation_text)\n",
        "            print(\"Validation score:\", validation_pp)\n",
        "\n",
        "            if validation_pp < prev_validation:\n",
        "                torch.save(self.network.state_dict(), \"neural_language_model.pkl\")\n",
        "                prev_validation = validation_pp\n",
        "        \n",
        "        # load best saved model\n",
        "        self.network.load_state_dict(torch.load(\"neural_language_model.pkl\"))\n",
        "    \n",
        "    def get_probability(self, text):\n",
        "        assert len(text) >= self.n, f\"Expected at least {self.n} words; got {len(text)} words. \\nGiven text: {text}\"\n",
        "        target_id = ids(self.vocab, [text[-1]])[0]\n",
        "        return self.get_next_word_probabilities(text[:-1])[target_id]\n",
        "\n",
        "    def get_next_word_probabilities(self, text_prefix):\n",
        "        self.network.eval()\n",
        "        while len(text_prefix) < self.n - 1:\n",
        "            text_prefix = [\"<eos>\"] + text_prefix\n",
        "        if len(text_prefix) > self.n - 1:\n",
        "            text_prefix = text_prefix[len(text_prefix) - self.n + 1 :]\n",
        "        x = torch.Tensor(ids(self.vocab, text_prefix)).to(torch.int64).to(device)\n",
        "        x = x.reshape((1, len(x)))\n",
        "        with torch.no_grad():\n",
        "            probs = F.softmax(self.network(x), dim=1)[0]\n",
        "        return probs\n",
        "\n",
        "neural_trigram_model = NeuralNGramModel(3, vocab)\n",
        "neural_trigram_model.train()\n",
        "print('neural trigram validation perplexity:', get_perplexity(neural_trigram_model, validation_text))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H8jFjmrTcFIJ"
      },
      "source": [
        "### RNN Model\n",
        "\n",
        "Now, we will train a Recurrnt Neural Network to model our language. This is a much more flexible architecture for language modeling since it is able to handle inputs of any length and can thus model longer ranges of text."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y4W-4ExjdFCS"
      },
      "outputs": [],
      "source": [
        "num_hidden_rnn_layers = 1\n",
        "class RNNNetwork(nn.Module):\n",
        "\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "\n",
        "        self.rnn = nn.RNN(128, 128, num_hidden_rnn_layers, dropout=0.5).to(device)\n",
        "        self.linear = nn.Linear(128, 128).to(device)\n",
        "        self.linear_2 = nn.Linear(128, vocab_size).to(device)\n",
        "        self.dp = nn.Dropout(0.5)\n",
        "\n",
        "    def forward(self, x, state):\n",
        "        x = F.embedding(x, weight=self.linear_2.weight) # weight tying\n",
        "        x, state = self.rnn(x, state)\n",
        "        x = self.dp(x)\n",
        "        x = self.linear(x)\n",
        "        x = self.linear_2(x)\n",
        "        return x, state\n",
        "\n",
        "class RNNModel:\n",
        "\n",
        "    def __init__(self, vocab):\n",
        "        self.n = 2 # makes it compatible with other n-gram models\n",
        "        self.vocab = vocab\n",
        "        self.network = RNNNetwork().to(device)\n",
        "\n",
        "    def train(self):\n",
        "        train_iterator = torchtext.data.BPTTIterator(train_dataset, batch_size=64, \n",
        "                                                     bptt_len=32, device=device)\n",
        "  \n",
        "        h = torch.autograd.Variable(torch.zeros(num_hidden_rnn_layers, 64, self.network.rnn.hidden_size), requires_grad=False).to(device)\n",
        "        optim = torch.optim.Adam(self.network.parameters())\n",
        "        prev_validation = float('inf')\n",
        "        for epoch in range(20):\n",
        "          print('Epoch', epoch + 1)\n",
        "          self.network.train()\n",
        "          for batch in tqdm.notebook.tqdm(train_iterator, leave=False):\n",
        "            assert self.network.training, 'make sure your network is in train mode with `.train()`'\n",
        "            text, target = batch.text, batch.target\n",
        "            text, target = text.to(torch.int64).to(device), target.to(torch.int64).to(device)\n",
        "            optim.zero_grad()\n",
        "\n",
        "            output, h = self.network(text, h)\n",
        "            output = output.view(-1, output.shape[-1])\n",
        "            target = target.view(-1,)\n",
        "            loss = F.cross_entropy(output, target)\n",
        "            loss.backward()\n",
        "            optim.step()\n",
        "            h = h.detach()\n",
        "\n",
        "          validation_pp = get_perplexity(self, validation_text)\n",
        "          print('Validation score:', validation_pp)\n",
        "\n",
        "          if validation_pp < prev_validation:\n",
        "            torch.save(self.network.state_dict(), \"rnn_language_model.pkl\")\n",
        "            prev_validation = validation_pp\n",
        "\n",
        "        self.network.load_state_dict(torch.load(\"rnn_language_model.pkl\"))\n",
        "    \n",
        "    def get_probability(self, text):\n",
        "        target_id = ids(self.vocab, [text[-1]])[0]\n",
        "        return self.get_next_word_probabilities(text[-32:-1])[target_id]\n",
        "\n",
        "    def get_next_word_probabilities(self, text_prefix):\n",
        "        prefix_token_tensor = torch.tensor(ids(self.vocab, text_prefix), device=device).view(-1, 1)\n",
        "        prefix_token_tensor = prefix_token_tensor.to(torch.int64).to(device)\n",
        "        h = torch.autograd.Variable(next(self.network.parameters()).data.new(num_hidden_rnn_layers, 1, self.network.rnn.hidden_size), requires_grad=False)\n",
        "        self.network.eval()\n",
        "        with torch.no_grad():\n",
        "          output, _ = self.network(prefix_token_tensor, h)\n",
        "          output = output.squeeze(dim=1)\n",
        "          probs = F.softmax(output, dim=-1)\n",
        "        return probs[-1]\n",
        "\n",
        "rnn_model = RNNModel(vocab)\n",
        "rnn_model.train()\n",
        "print('rnn validation perplexity:', get_perplexity(rnn_model, validation_text))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6So9wKWlhIyl"
      },
      "source": [
        "## Submission\n",
        "\n",
        "Congrats on making it to the end of the notebook. Please download this notebook and upload it to gradescope. Make sure all of the cells where your answers are expected are filled in."
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "HW_4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.5"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "2bf4343c137c403190dafe0cba69a399": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_75ff119b5d834ac19da87c7e5f3501c5",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_96c5c17490024cc6a6ce012c306719fb",
              "IPY_MODEL_5263766dc3514d359c1181d5c2d48502",
              "IPY_MODEL_568f8576aa7d4ca5b50d79d8115504eb"
            ]
          }
        },
        "75ff119b5d834ac19da87c7e5f3501c5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "96c5c17490024cc6a6ce012c306719fb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_f4c51b7caf4844e19aa273e7446a1469",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": "100%",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_923aa292540c4e68b84985395bacc8f8"
          }
        },
        "5263766dc3514d359c1181d5c2d48502": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_f6433befc90f46d884d38acc0740b666",
            "_dom_classes": [],
            "description": "",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 42303,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 42303,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_e87f38713c044707bb71fa7b0851236b"
          }
        },
        "568f8576aa7d4ca5b50d79d8115504eb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_ba55617e8b0e4c728c6cff99de8f3dd6",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 42303/42303 [02:03&lt;00:00, 324.34it/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_a5c41eef61f74694932d3ba90d80b608"
          }
        },
        "f4c51b7caf4844e19aa273e7446a1469": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "923aa292540c4e68b84985395bacc8f8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "f6433befc90f46d884d38acc0740b666": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "e87f38713c044707bb71fa7b0851236b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "ba55617e8b0e4c728c6cff99de8f3dd6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "a5c41eef61f74694932d3ba90d80b608": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}